{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "E7m9JjNrNbYy"
   },
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from src.classes.Dataset import MRIDataset, MRISubset\n",
    "from src.classes.Models import ResNet50variant, ResNet18variant\n",
    "from src.config import PATH_TO_DATASET, PATH_TO_DATASET_CSV, PATH_TO_OUTPUT\n",
    "from src.functions.train_eval import train_model, train_final_model, evaluate, evaluate_img\n",
    "from src.functions.utils_train import oversampling, plot_results, class_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training and Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ACcemFRwNgst"
   },
   "source": [
    "# Define input paths\n",
    "input_path = PATH_TO_DATASET\n",
    "df = pd.read_csv(PATH_TO_DATASET_CSV, sep=';', header=0)\n",
    "\n",
    "# Define class names and mapping\n",
    "class_names = ['healthy', 'affected']\n",
    "id2name = {idx: c for idx, c in enumerate(class_names)}\n",
    "\n",
    "# Generate dataset mapping\n",
    "data = {idx: (os.path.join(input_path, id2name[row['label']], str(row['img_name'])), row['label']) for idx, row in\n",
    "        df.iterrows()}\n",
    "\n",
    "# Convert labels and groups to numpy arrays\n",
    "y = df['label'].to_numpy()\n",
    "groups = df['group'].to_numpy()\n",
    "\n",
    "# Set up cross-validation with stratified group splitting\n",
    "k_folds = 5\n",
    "sgkf = StratifiedGroupKFold(n_splits=k_folds, shuffle=True, random_state=7)\n",
    "X = np.array(list(data.keys()))\n",
    "\n",
    "# Perform train-test split\n",
    "train_index, test_index = next(sgkf.split(X, y, groups))\n",
    "\n",
    "# Print dataset distribution information\n",
    "print(f\"Train groups: {set(groups[train_index])}, Test groups: {set(groups[test_index])}\")\n",
    "print(f\"Dataset size: {len(data)}\")\n",
    "print(\n",
    "    f\"Train size: {len(train_index)}, Affected: {np.count_nonzero(y[train_index] == 1)}, Healthy: {np.count_nonzero(y[train_index] == 0)}\")\n",
    "print(\n",
    "    f\"Test size: {len(test_index)}, Affected: {np.count_nonzero(y[test_index] == 1)}, Healthy: {np.count_nonzero(y[test_index] == 0)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YSZ8egC0Ngu3"
   },
   "source": [
    "# Define training parameters\n",
    "k_folds = 10\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 100\n",
    "patience = 5\n",
    "delta = 0.006\n",
    "title = 'train-resnet50v'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.GaussianBlur(5, sigma=(0.1, 0.5)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.ToDtype(torch.float32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToDtype(torch.float32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = MRIDataset(data)\n",
    "\n",
    "# Define class mapping\n",
    "class_names = ['healthy', 'affected']\n",
    "id2name = {idx: c for idx, c in enumerate(class_names)}\n",
    "num_classes = 2\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize metrics\n",
    "accuracy, precision, recall, f1, training_epochs = [], [], [], [], []\n",
    "\n",
    "# Set up Stratified Group K-Fold cross-validation\n",
    "sgkf = StratifiedGroupKFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
    "figure, axis = plt.subplots(k_folds, 2, figsize=(20, 30))\n",
    "figure.tight_layout(pad=5.0)\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (new_train_index, valid_index) in enumerate(sgkf.split(X[train_index], y[train_index], groups[train_index])):\n",
    "    # Perform oversampling\n",
    "    new_train_index_resampled, new_train_y_resampled = oversampling(\n",
    "        X[train_index][new_train_index], y[train_index][new_train_index], seed=fold, sampling_strategy=1\n",
    "    )\n",
    "\n",
    "    print(f'\\n--------------------------------\\nFOLD {fold}\\n--------------------------------')\n",
    "    print(\n",
    "        f'Training set: {len(new_train_index_resampled)}, Validation set: {len(valid_index)}, Test set: {len(test_index)}')\n",
    "    print(\n",
    "        f'Training + Validation - Affected: {np.count_nonzero(y[train_index][valid_index] == 1) + np.count_nonzero(new_train_y_resampled == 1)}, Healthy: {np.count_nonzero(y[train_index][valid_index] == 0) + np.count_nonzero(new_train_y_resampled == 0)}')\n",
    "    print(\n",
    "        f'Training set - Affected: {np.count_nonzero(new_train_y_resampled == 1)}, Healthy: {np.count_nonzero(new_train_y_resampled == 0)}')\n",
    "    print(\n",
    "        f'Validation set - Affected: {np.count_nonzero(y[train_index][valid_index] == 1)}, Healthy: {np.count_nonzero(y[train_index][valid_index] == 0)}')\n",
    "    print(\n",
    "        f'Test set - Affected: {np.count_nonzero(y[test_index] == 1)}, Healthy: {np.count_nonzero(y[test_index] == 0)}')\n",
    "\n",
    "    # Prepare datasets and dataloaders\n",
    "    train = MRISubset(Subset(dataset, new_train_index_resampled), train_bool=True, transform=train_transforms)\n",
    "    valid = MRISubset(Subset(dataset, X[train_index][valid_index]), train_bool=False, transform=test_transforms)\n",
    "    datasets = {'train': train, 'valid': valid}\n",
    "    dataloaders = {x: DataLoader(datasets[x], batch_size=32, shuffle=True) for x in ['train', 'valid']}\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = ResNet50variant().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    model, res, history = train_model(fold, model, criterion, optimizer, epochs, patience, delta, title)\n",
    "    plot_results(history, axis, fold)\n",
    "\n",
    "    # Store results\n",
    "    accuracy.append(accuracy_score(res['labels'], res['preds']))\n",
    "    precision.append(precision_score(res['labels'], res['preds']))\n",
    "    recall.append(recall_score(res['labels'], res['preds']))\n",
    "    f1.append(f1_score(res['labels'], res['preds']))\n",
    "    training_epochs.append(history['epochs'])\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\n",
    "    f'\\n--------------------------------\\nK-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS\\n--------------------------------')\n",
    "for fold in range(k_folds):\n",
    "    print(\n",
    "        f'Fold {fold}: Accuracy: {accuracy[fold]}, Precision: {precision[fold]}, Recall: {recall[fold]}, F1: {f1[fold]}')\n",
    "\n",
    "print(\n",
    "    f'Average accuracy: {np.mean(accuracy)}, Average precision: {np.mean(precision)}, Average recall: {np.mean(recall)}, Average F1: {np.mean(f1)}')\n",
    "print(f'Average training epochs: {np.mean(training_epochs)}')\n",
    "print(\n",
    "    f'Variance of accuracy: {np.var(accuracy)}, Variance of precision: {np.var(precision)}, Variance of recall: {np.var(recall)}, Variance of F1: {np.var(f1)}')\n",
    "\n",
    "# Save results\n",
    "figure.savefig(f'{input_path}/models/{title}/plot.png')\n",
    "\n",
    "results = {\n",
    "    'title': title,\n",
    "    'learning rate': learning_rate,\n",
    "    'momentum': momentum,\n",
    "    'patience': patience,\n",
    "    'delta': delta,\n",
    "    'final': {'accuracy': np.mean(accuracy), 'precision': np.mean(precision), 'recall': np.mean(recall),\n",
    "              'f1': np.mean(f1), 'avg epochs': np.mean(training_epochs)},\n",
    "    'variance': {'accuracy': np.var(accuracy), 'precision': np.var(precision), 'recall': np.var(recall),\n",
    "                 'f1': np.var(f1)}\n",
    "}\n",
    "for fold in range(k_folds):\n",
    "    results[str(fold)] = {'accuracy': accuracy[fold], 'precision': precision[fold], 'recall': recall[fold],\n",
    "                          'f1': f1[fold]}\n",
    "\n",
    "with open(f\"{input_path}/models/{title}/results.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile)\n",
    "\n",
    "avg_epochs = np.mean(training_epochs)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vOP6KO-jNgyV"
   },
   "source": [
    "# Final model\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "epochs = round(avg_epochs)  # Hyperparameter\n",
    "\n",
    "title = 'final-resnet50v'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Augmentation for Training and Testing\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.GaussianBlur(5, sigma=(0.1, 0.5)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.ToDtype(torch.float32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToDtype(torch.float32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = MRIDataset(data)\n",
    "\n",
    "# Mapping class labels to names\n",
    "class_names = ['healthy', 'affected']\n",
    "id2name = {idx: c for idx, c in enumerate(class_names)}\n",
    "\n",
    "num_classes = 2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "# Oversampling\n",
    "train_index_resampled, train_y_resampled = oversampling(X[train_index], y[train_index], seed=0, sampling_strategy=1)\n",
    "\n",
    "# Dataset Information\n",
    "print('Dataset length:', len(data))\n",
    "print('Training set length:', len(train_index))\n",
    "print('Affected cases in training set:', np.count_nonzero(y[train_index] == 1))\n",
    "print('Healthy cases in training set:', np.count_nonzero(y[train_index] == 0), '\\n')\n",
    "\n",
    "print('Resampled training set length:', len(train_index_resampled))\n",
    "print('Affected cases in resampled training set:', np.count_nonzero(y[train_index_resampled] == 1))\n",
    "print('Healthy cases in resampled training set:', np.count_nonzero(y[train_index_resampled] == 0), '\\n')\n",
    "\n",
    "print('Test set length:', len(test_index))\n",
    "print('Affected cases in test set:', np.count_nonzero(y[test_index] == 1))\n",
    "print('Healthy cases in test set:', np.count_nonzero(y[test_index] == 0), '\\n')\n",
    "\n",
    "# DataLoader Setup\n",
    "train = MRISubset(Subset(dataset, train_index_resampled), train_bool=True, transform=train_transforms)\n",
    "test = MRISubset(Subset(dataset, test_index), train_bool=False, transform=test_transforms)\n",
    "\n",
    "datasets = {'train': train, 'test': test}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "dataloaders = {\n",
    "    x: DataLoader(datasets[x], batch_size=32, shuffle=True) if x == 'train' else DataLoader(datasets[x], batch_size=32,\n",
    "                                                                                            shuffle=False) for x in\n",
    "    ['train', 'test']\n",
    "}\n",
    "\n",
    "# Model Setup\n",
    "resnet50v = ResNet50variant()\n",
    "resnet50v = resnet50v.to(device)\n",
    "\n",
    "optimizer = optim.SGD(resnet50v.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Training the Model\n",
    "resnet50v = train_final_model(resnet50v, criterion, optimizer, epochs, title)\n",
    "\n",
    "# Model Evaluation\n",
    "res = evaluate(resnet50v, dataloaders['test'])\n",
    "class_results(res)\n",
    "\n",
    "# Print Evaluation Metrics\n",
    "print()\n",
    "print(f'Accuracy: {accuracy_score(res[\"labels\"], res[\"preds\"])}')\n",
    "print(f'Precision: {precision_score(res[\"labels\"], res[\"preds\"])}')\n",
    "print(f'Recall: {recall_score(res[\"labels\"], res[\"preds\"])}')\n",
    "print(f'F1: {f1_score(res[\"labels\"], res[\"preds\"])}')\n",
    "\n",
    "# Save Results to File\n",
    "results = {}\n",
    "results['title'] = title\n",
    "results['learning rate'] = learning_rate\n",
    "results['momentum'] = momentum\n",
    "results['epochs'] = epochs\n",
    "\n",
    "# Final Stats\n",
    "stats = {}\n",
    "stats['accuracy'] = accuracy_score(res[\"labels\"], res[\"preds\"])\n",
    "stats['precision'] = precision_score(res[\"labels\"], res[\"preds\"])\n",
    "stats['recall'] = recall_score(res[\"labels\"], res[\"preds\"])\n",
    "stats['f1'] = f1_score(res[\"labels\"], res[\"preds\"])\n",
    "results['final'] = stats\n",
    "\n",
    "# Saving results to JSON\n",
    "with open(f\"{input_path}/models/{title}/results.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "## SHAP"
   ],
   "metadata": {
    "id": "s2twnSSLdaZq"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prediction function\n",
    "def my_predict(img: np.ndarray) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    img = nhwc_to_nchw(torch.Tensor(img))  # Convert NHWC to NCHW format\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    return output\n",
    "\n",
    "# Conversion from NCHW to NHWC\n",
    "def nchw_to_nhwc(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        return x if x.shape[3] == 3 else x.permute(0, 2, 3, 1)\n",
    "    elif x.dim() == 3:\n",
    "        return x if x.shape[2] == 3 else x.permute(1, 2, 0)\n",
    "    return x\n",
    "\n",
    "# Conversion from NHWC to NCHW\n",
    "def nhwc_to_nchw(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        return x if x.shape[1] == 3 else x.permute(0, 3, 1, 2)\n",
    "    elif x.dim() == 3:\n",
    "        return x if x.shape[0] == 3 else x.permute(2, 0, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IwZaWFaCn45G"
   },
   "source": [
    "# Hyperparameters\n",
    "topk = 1\n",
    "batch_size = 50\n",
    "n_evals = 5000\n",
    "\n",
    "input_path = PATH_TO_DATASET\n",
    "output_path = PATH_TO_OUTPUT\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define test transformations\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToDtype(torch.float32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset and Subset creation\n",
    "dataset = MRIDataset(data)\n",
    "test = MRISubset(Subset(dataset, test_index), train_bool=False, transform=test_transforms)\n",
    "\n",
    "# Class names mapping\n",
    "class_names = ['healthy', 'affected']\n",
    "\n",
    "# Prepare shap values for all test samples\n",
    "shap_test = []\n",
    "for i in range(len(test)):\n",
    "    img, _ = test[i]  # Get image (numpy array)\n",
    "    shap_test.append(img)\n",
    "\n",
    "shap_test = torch.stack(shap_test)\n",
    "shap_test = nchw_to_nhwc(shap_test)\n",
    "\n",
    "# Recreate output folder if it exists\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "os.mkdir(output_path)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = ResNet50variant()\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{input_path}/models/final-resnet50v/saved_model-final.pth\", map_location=torch.device('cpu'))\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Masker for image\n",
    "masker_blur = shap.maskers.Image(\"blur(64,64)\", shap_test[0].shape)\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(mypredict, masker_blur, output_names=class_names, seed=11)\n",
    "\n",
    "# Get image names from the dataset\n",
    "img_names = [data[k][0].split('/')[-1] for k in test_index]\n",
    "\n",
    "# Store shap values and generate plots\n",
    "shap_data = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    model.eval()\n",
    "    input, _ = test[i]\n",
    "    input = input.to(device)\n",
    "\n",
    "    # Predict class\n",
    "    predicted_class_index = torch.argmax(model(input.unsqueeze(0)).detach(), dim=1).item()\n",
    "\n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer(\n",
    "        shap_test[i].unsqueeze(0),\n",
    "        max_evals=n_evals,\n",
    "        batch_size=batch_size,\n",
    "        outputs=shap.Explanation.argsort.flip[:topk]\n",
    "    )\n",
    "\n",
    "    shap_values.data = shap_values.data.cpu().numpy()\n",
    "    shap_data.append(shap_values)\n",
    "\n",
    "    # Create subdirectory based on predicted class\n",
    "    subdir = id2name[predicted_class_index]\n",
    "    final_output_path = os.path.join(output_path, subdir)\n",
    "    os.makedirs(final_output_path, exist_ok=True)\n",
    "\n",
    "    # Plot SHAP image\n",
    "    shap.image_plot(shap_values, show=False)\n",
    "    plt.savefig(f'{final_output_path}/{img_names[i]}')\n",
    "\n",
    "    # Save SHAP values as numpy arrays\n",
    "    numpy_output_path = os.path.join(output_path, 'shap_values')\n",
    "    os.makedirs(numpy_output_path, exist_ok=True)\n",
    "    np.save(os.path.join(numpy_output_path, f'{img_names[i]}.npy'), shap_values.values)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Ensemble Method",
   "metadata": {
    "id": "iYnRFOgtl8Xx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to discretize an array with a sliding window\n",
    "def discretize(arr: np.ndarray, window_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Discretizes an image (2D array) by averaging values in a sliding window.\n",
    "\n",
    "    :param arr: Input 2D numpy array.\n",
    "    :param window_size: Size of the window used to average the values.\n",
    "    :return: 2D numpy array with averaged values.\n",
    "    \"\"\"\n",
    "    H, W = arr.shape\n",
    "    output = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    # Iterate over the array in steps of window_size\n",
    "    for i in range(0, H, window_size):\n",
    "        y_end = min(i + window_size, H)\n",
    "        for j in range(0, W, window_size):\n",
    "            x_end = min(j + window_size, W)\n",
    "            output[i:y_end, j:x_end] = np.mean(arr[i:y_end, j:x_end])\n",
    "\n",
    "    return output\n",
    "\n",
    "# Function to add text to an image\n",
    "def add_text_to_image(img: np.ndarray, text: str, space: int = 10,\n",
    "                      color: tuple = (255, 255, 255), thickness: int = 2,\n",
    "                      scale: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds centered text to an image.\n",
    "\n",
    "    :param img: Input image (numpy array).\n",
    "    :param text: The text to be added.\n",
    "    :param space: Space from the top of the image for the text.\n",
    "    :param color: Color of the text in BGR format (default is white).\n",
    "    :param thickness: Thickness of the text (default is 2).\n",
    "    :param scale: Scale of the text (default is 0.5).\n",
    "    :return: The image with added text.\n",
    "    \"\"\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_size, _ = cv2.getTextSize(text, font, scale, thickness)\n",
    "    text_width, text_height = text_size\n",
    "\n",
    "    # Calculate position to center text horizontally and place it vertically\n",
    "    height, width, _ = img.shape\n",
    "    x = (width - text_width) // 2\n",
    "    y = text_height + space if space else text_height + 10  # Top margin\n",
    "\n",
    "    # Add the text to the image\n",
    "    cv2.putText(img, text, (x, y), font, scale, color, thickness)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Function to normalize an array to a range between 0 and 1\n",
    "def normalize(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes the array values to the range [0, 1].\n",
    "\n",
    "    :param arr: Input 2D or 1D numpy array.\n",
    "    :return: Normalized numpy array.\n",
    "    \"\"\"\n",
    "    if np.any(arr):\n",
    "        arr = np.maximum(0, arr)  # Ensure non-negative values\n",
    "        arr = arr - arr.min()  # Shift values to start from 0\n",
    "        return arr / (arr.max() - arr.min())  # Scale to [0, 1]\n",
    "    return arr  # Return the input array if it is empty or all zeros\n"
   ],
   "metadata": {
    "id": "1HTzKjNCqouD"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Utility function to apply threshold and normalize values\n",
    "def threshold(img: np.ndarray, res: list, thres: float, binary: bool = False, size: int = 14) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies a threshold to the ensemble of results and returns a combined output.\n",
    "\n",
    "    :param img: The input image (not used directly in the function).\n",
    "    :param res: List of result arrays to combine.\n",
    "    :param thres: Threshold value for inclusion.\n",
    "    :param binary: Whether to return a binary mask (default: False).\n",
    "    :param size: Size parameter (not used directly).\n",
    "    :return: Combined output array, either binary or normalized.\n",
    "    \"\"\"\n",
    "    H, W = res[0].shape\n",
    "    output = np.zeros((H, W), dtype=float)\n",
    "    mask = np.zeros((H, W), dtype=float)\n",
    "\n",
    "    # Apply thresholding across each result array\n",
    "    for arr in res:\n",
    "        normalized_arr = normalize(arr)\n",
    "        mask += np.where(normalized_arr >= thres, 1, 0)\n",
    "        output += np.where(normalized_arr >= thres, normalized_arr, 0)\n",
    "\n",
    "    output = output / len(res)\n",
    "    majority = np.where(mask >= len(res) - 1, 1, 0)\n",
    "\n",
    "    return majority if binary else normalize(output) * majority\n",
    "\n",
    "\n",
    "# Utility function to average the results of multiple arrays\n",
    "def average(res: list, binary: bool = False, size: int = 14) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Averages the results across multiple arrays.\n",
    "\n",
    "    :param res: List of result arrays to average.\n",
    "    :param binary: Whether to return a binary mask (default: False).\n",
    "    :param size: Size parameter (not used directly).\n",
    "    :return: Averaged result array, either binary or normalized.\n",
    "    \"\"\"\n",
    "    H, W = res[0].shape\n",
    "    output = np.zeros((H, W), dtype=float)\n",
    "    mask = np.zeros((H, W), dtype=float)\n",
    "\n",
    "    # Average the values across the result arrays\n",
    "    for arr in res:\n",
    "        normalized_arr = normalize(arr)\n",
    "        output += np.where(normalized_arr > 0, normalized_arr, 0)\n",
    "        mask += np.where(normalized_arr > 0, 1, 0)\n",
    "\n",
    "    output = output / len(res)\n",
    "    majority = np.where(mask >= len(res) / 2, 1, 0)\n",
    "\n",
    "    if binary:\n",
    "        return majority\n",
    "    else:\n",
    "        output = output * majority\n",
    "        return np.where(output > 0.5, output, 0)\n",
    "\n",
    "\n",
    "# Utility function to find intersection of multiple result arrays\n",
    "def intersection(res: list, binary: bool = False, size: int = 14) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the intersection of results from multiple arrays.\n",
    "\n",
    "    :param res: List of result arrays to combine.\n",
    "    :param binary: Whether to return a binary mask (default: False).\n",
    "    :param size: Size parameter (not used directly).\n",
    "    :return: Intersection result array, either binary or normalized.\n",
    "    \"\"\"\n",
    "    H, W = res[0].shape\n",
    "    output = np.zeros((H, W), dtype=float)\n",
    "    mask = np.zeros((H, W), dtype=float)\n",
    "\n",
    "    # Calculate intersection by considering values above a certain threshold\n",
    "    for arr in res:\n",
    "        normalized_arr = normalize(arr)\n",
    "        output += np.where(normalized_arr > 0.2, normalized_arr, 0)\n",
    "        mask += np.where(normalized_arr > 0.2, 1, 0)\n",
    "\n",
    "    output = output / len(res)\n",
    "    majority = np.where(mask == len(res), 1, 0)\n",
    "\n",
    "    if binary:\n",
    "        return majority\n",
    "    else:\n",
    "        output = output * majority\n",
    "        return np.where(output, output, 0)\n"
   ],
   "metadata": {
    "id": "YKRi44V-l6w7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Clear and create output directory\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "os.mkdir(output_path)\n",
    "\n",
    "# Image and label initialization\n",
    "img_names = []\n",
    "labels = []\n",
    "for k in test_index:\n",
    "    name, label = data[k]\n",
    "    labels.append(label)\n",
    "    subname = name.split('/')\n",
    "    img_names.append(subname[-1])\n",
    "\n",
    "# Model initialization\n",
    "m18 = ResNet18variant()\n",
    "m18.load_state_dict(torch.load('drive/MyDrive/Tesi/data_tif_no_background/models/final-resnet18v/saved_model-final.pth', map_location=torch.device(device)))\n",
    "m18 = m18.to(device)\n",
    "\n",
    "m50 = ResNet50variant()\n",
    "m50.load_state_dict(torch.load('drive/MyDrive/Tesi/data_tif_no_background/models/final-resnet50v/saved_model-final.pth', map_location=torch.device(device)))\n",
    "m50 = m50.to(device)\n",
    "\n",
    "models = [('Resnet18v', m18), ('Resnet50v', m50)]\n",
    "\n",
    "# Set size for discretization\n",
    "size = 7\n",
    "\n",
    "# Process each image and generate ensemble results\n",
    "for n, name in enumerate(img_names):\n",
    "\n",
    "    # Process each model's predictions and explainability\n",
    "    for m, model in models:\n",
    "\n",
    "        img_orig, _ = test.__getitem__(n)\n",
    "        rgb_img = img_orig.repeat(3, 1, 1).numpy().transpose((1, 2, 0))\n",
    "\n",
    "        pred = evaluate_img(model, img_orig.unsqueeze(0)).item()\n",
    "\n",
    "        # Load GradCAM, GradCAM++, HiResCAM, Shap5000, and Occlusion maps\n",
    "        gradcam = np.load(os.path.join(input_path, \"cam_array\", m, \"GradCAM\", img_names[n] + '.npy'))\n",
    "        gradcampp = np.load(os.path.join(input_path, \"cam_array\", m, \"GradCAMPlusPlus\", img_names[n] + '.npy'))\n",
    "        hirescam = np.load(os.path.join(input_path, \"cam_array\", m, \"HiResCAM\", img_names[n] + '.npy'))\n",
    "\n",
    "        shap5000 = np.squeeze(np.load(os.path.join(input_path, \"Shap\", f\"{m}-5000\", \"shap_values\", img_names[n] + '.npy'))[:, :, :, :, 0][0])\n",
    "\n",
    "        # Determine Occlusion map based on prediction\n",
    "        occlusion_file = os.path.join(input_path, \"Occlusion_res\", m, f\"{img_names[n]}.npy\" if pred == 1 else f\"{img_names[n]}-2.npy\")\n",
    "        occlusion = np.load(occlusion_file)\n",
    "\n",
    "        # Determine explainability components\n",
    "        explainability = [discretize(gradcam, size), discretize(gradcampp, size), discretize(hirescam, size), discretize(shap5000, size)]\n",
    "        if np.any(occlusion >= 1):\n",
    "            explainability.append(discretize(occlusion, size))\n",
    "\n",
    "        # Call ensemble algorithm to combine the explainability maps\n",
    "        res = ensemble_algorithm(explainability)  # Replace with your ensemble function\n",
    "\n",
    "        # Generate heatmap from ensemble results\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * res), cv2.COLORMAP_PARULA)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "        # Blend heatmap with original image\n",
    "        final_img = cv2.addWeighted(rgb_img, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "        # Prepare output directory structure\n",
    "        subdir = id2name[labels[n]]\n",
    "        final_output_path = os.path.join(output_path, m, subdir)\n",
    "\n",
    "        # Create necessary directories if they don't exist\n",
    "        os.makedirs(final_output_path, exist_ok=True)\n",
    "\n",
    "        # Save the final image\n",
    "        final_img = cv2.cvtColor(final_img, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(f'{final_output_path}/{img_names[n]}', final_img)\n"
   ],
   "metadata": {
    "id": "qsOyfyceNLLF"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
